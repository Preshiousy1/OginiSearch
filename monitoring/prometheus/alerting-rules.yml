groups:
  - name: ogini-postgresql-alerts
    rules:
      # === DATABASE CONNECTION ALERTS ===
      - alert: PostgreSQLHighConnections
        expr: pg_stat_database_numbackends > 400
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'High PostgreSQL connections ({{ $value }})'
          description: 'PostgreSQL has {{ $value }} active connections'

      - alert: PostgreSQLConnectionLimit
        expr: pg_stat_database_numbackends / pg_settings_max_connections > 0.8
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: 'PostgreSQL connection limit approaching ({{ $value | humanizePercentage }})'
          description: 'PostgreSQL connections at {{ $value | humanizePercentage }} of limit'

      # === PERFORMANCE ALERTS ===
      - alert: PostgreSQLSlowQueries
        expr: rate(pg_stat_activity_max_tx_duration[5m]) > 30
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'Slow PostgreSQL queries detected'
          description: 'Average query duration is {{ $value }}s'

      - alert: PostgreSQLHighCPU
        expr: rate(process_cpu_seconds_total{job="postgresql"}[5m]) * 100 > 80
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: 'High PostgreSQL CPU usage ({{ $value | humanizePercentage }})'
          description: 'PostgreSQL CPU usage is {{ $value | humanizePercentage }}'

      - alert: PostgreSQLHighMemory
        expr: (process_resident_memory_bytes{job="postgresql"} / 8589934592) * 100 > 85
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'High PostgreSQL memory usage ({{ $value | humanizePercentage }})'
          description: 'PostgreSQL memory usage is {{ $value | humanizePercentage }}'

      # === DISK SPACE ALERTS ===
      - alert: PostgreSQLLowDiskSpace
        expr: (node_filesystem_avail_bytes{mountpoint="/var/lib/postgresql/data"} / node_filesystem_size_bytes{mountpoint="/var/lib/postgresql/data"}) * 100 < 15
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: 'Low PostgreSQL disk space ({{ $value | humanizePercentage }})'
          description: 'PostgreSQL data directory has {{ $value | humanizePercentage }} free space'

      # === REPLICATION ALERTS ===
      - alert: PostgreSQLReplicationLag
        expr: pg_replication_lag_bytes > 1000000000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'PostgreSQL replication lag detected'
          description: 'Replication lag is {{ $value | humanize }} bytes'

      # === LOCK ALERTS ===
      - alert: PostgreSQLLockWait
        expr: pg_stat_activity_wait_event_type == 'Lock'
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: 'PostgreSQL lock wait detected'
          description: 'Queries waiting for locks'

  - name: ogini-application-alerts
    rules:
      # === APPLICATION PERFORMANCE ===
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'High response time (p95: {{ $value }}s)'
          description: '95th percentile response time is {{ $value }}s'

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: 'High error rate ({{ $value | humanizePercentage }})'
          description: 'Error rate is {{ $value | humanizePercentage }}'

      - alert: HighSearchLatency
        expr: histogram_quantile(0.95, rate(search_duration_seconds_bucket[5m])) > 1.0
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: 'High search latency (p95: {{ $value }}s)'
          description: '95th percentile search latency is {{ $value }}s'

      # === QUEUE ALERTS ===
      - alert: HighQueueSize
        expr: indexing_queue_size > 1000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'High indexing queue size ({{ $value }})'
          description: 'Indexing queue has {{ $value }} pending jobs'

      - alert: QueueProcessingStuck
        expr: rate(indexing_queue_processed_total[5m]) == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: 'Indexing queue processing stopped'
          description: 'No jobs processed in the last 5 minutes'

      # === MEMORY ALERTS ===
      - alert: HighMemoryUsage
        expr: (process_resident_memory_bytes / 8589934592) * 100 > 85
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'High application memory usage ({{ $value | humanizePercentage }})'
          description: 'Application memory usage is {{ $value | humanizePercentage }}'

      # === WORKER ALERTS ===
      - alert: WorkerFailure
        expr: rate(worker_failures_total[5m]) > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: 'Worker failures detected'
          description: '{{ $value }} worker failures per second'

      - alert: LowWorkerCount
        expr: active_workers < 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'Low worker count ({{ $value }})'
          description: 'Only {{ $value }} workers are active'

  - name: ogini-infrastructure-alerts
    rules:
      # === SYSTEM ALERTS ===
      - alert: HighSystemLoad
        expr: node_load1 > 0.8
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: 'High system load ({{ $value }})'
          description: 'System load average is {{ $value }}'

      - alert: HighDiskIO
        expr: rate(node_disk_io_time_seconds_total[5m]) > 0.8
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'High disk I/O ({{ $value | humanizePercentage }})'
          description: 'Disk I/O utilization is {{ $value | humanizePercentage }}'

      - alert: HighNetworkTraffic
        expr: rate(node_network_receive_bytes_total[5m]) + rate(node_network_transmit_bytes_total[5m]) > 1000000000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'High network traffic ({{ $value | humanize }}B/s)'
          description: 'Network traffic is {{ $value | humanize }}B/s'

      # === SERVICE ALERTS ===
      - alert: ServiceDown
        expr: up == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: 'Service {{ $labels.job }} is down'
          description: 'Service {{ $labels.job }} has been down for more than 30 seconds'

      - alert: ServiceHighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'Service {{ $labels.job }} high latency (p95: {{ $value }}s)'
          description: '95th percentile latency is {{ $value }}s'
